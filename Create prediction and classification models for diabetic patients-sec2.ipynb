{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "724acd92",
   "metadata": {},
   "source": [
    "### Assignment 1\n",
    "### Student ID: 34116478\n",
    "### Student Name: Qi Zhou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e87381d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import zero_one_loss, log_loss\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08afa6b7",
   "metadata": {},
   "source": [
    "# S3.  Ridge Regression and Logistic Regression versus Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601046dd",
   "metadata": {},
   "source": [
    "## Question 4: Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce51a301",
   "metadata": {},
   "source": [
    "# 4.1 Derive the weight update steps of stochastic gradient descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a37cdf",
   "metadata": {},
   "source": [
    "# Derivation of Weight Update Steps for Stochastic Gradient Descent (SGD) with L2 Regularization\n",
    "\n",
    "To derive the weight update steps for linear regression with L2 regularization using stochastic gradient descent (SGD), we start by defining the regularized error function and proceed to derive the gradient, which will be used to update the weights.\n",
    "\n",
    "## 1. Regularized Error Function\n",
    "\n",
    "The regularized error function $E(\\mathbf{w})$ for linear regression with L2 regularization is defined as follows:\n",
    "\n",
    "$$\n",
    "E(\\mathbf{w}) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left( t_i - \\mathbf{x}_i \\cdot \\mathbf{w} \\right)^2 + \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $n$ is the number of training samples.\n",
    "- $t_i$ is the true target value for the $i$-th sample.\n",
    "- $\\mathbf{x}_i$ is the feature vector for the $i$-th sample.\n",
    "- $\\mathbf{w}$ is the weight vector (the model parameters).\n",
    "- $\\lambda$ is the regularization parameter controlling the strength of the L2 penalty.\n",
    "\n",
    "The first term in the equation is the mean squared error (MSE) of the linear regression model, and the second term is the L2 regularization term, also known as the ridge penalty.\n",
    "\n",
    "## 2. Gradient of the Regularized Error Function\n",
    "\n",
    "To find the weight update rule, we need to compute the gradient of the regularized error function with respect to the weight vector $\\mathbf{w}$. The gradient of $E(\\mathbf{w})$ is given by:\n",
    "\n",
    "$$\n",
    "\\nabla E(\\mathbf{w}) = -\\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{x}_i \\left( t_i - \\mathbf{x}_i \\cdot \\mathbf{w} \\right) + \\lambda \\mathbf{w}\n",
    "$$\n",
    "\n",
    "The first term in the gradient is the gradient of the MSE, and the second term is the gradient of the L2 regularization term.\n",
    "\n",
    "## 3. Stochastic Gradient Descent (SGD) Weight Update Rule\n",
    "\n",
    "In stochastic gradient descent, we do not compute the gradient over the entire dataset at once (as in batch gradient descent), but rather compute it for a single example or a small batch of examples. For a single example $(\\mathbf{x}_i, t_i)$, the gradient becomes:\n",
    "\n",
    "$$\n",
    "\\mathbf{g}_i(\\mathbf{w}) = -\\mathbf{x}_i \\left( t_i - \\mathbf{x}_i \\cdot \\mathbf{w} \\right) + \\lambda \\mathbf{w}\n",
    "$$\n",
    "\n",
    "Using this gradient, the weight update rule in SGD can be expressed as:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{\\tau} = \\mathbf{w}_{\\tau-1} - \\eta \\left( -\\mathbf{x}_i \\left( t_i - \\mathbf{x}_i \\cdot \\mathbf{w}_{\\tau-1} \\right) + \\lambda \\mathbf{w}_{\\tau-1} \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathbf{w}_{\\tau-1}$ is the weight vector at the previous iteration.\n",
    "- $\\eta$ is the learning rate.\n",
    "\n",
    "Simplifying the weight update rule:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{\\tau} = \\mathbf{w}_{\\tau-1} + \\eta \\mathbf{x}_i \\left( t_i - \\mathbf{x}_i \\cdot \\mathbf{w}_{\\tau-1} \\right) - \\eta \\lambda \\mathbf{w}_{\\tau-1}\n",
    "$$\n",
    "\n",
    "This update rule consists of two parts:\n",
    "\n",
    "1. **Gradient of the error function**: $\\eta \\mathbf{x}_i \\left( t_i - \\mathbf{x}_i \\cdot \\mathbf{w}_{\\tau-1} \\right)$\n",
    "2. **Regularization penalty**: $- \\eta \\lambda \\mathbf{w}_{\\tau-1}$\n",
    "\n",
    "The first part adjusts the weights to minimize the prediction error, and the second part penalizes large weights to prevent overfitting.\n",
    "\n",
    "## 4. System of Linear Equations\n",
    "\n",
    "Alternatively, we can find the optimal weight vector \\( w^* \\) by solving a system of linear equations derived from setting the gradient of the regularized error function to zero. This leads to the following normal equation:\n",
    "\n",
    "$$\n",
    "X^\\top X w^* + \\lambda w^* = X^\\top y\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "-  X  is the matrix of input features.\n",
    "-  y  is the vector of target values.\n",
    "\n",
    "Solving this system of equations gives us the weight vector that minimizes the regularized error function. The regularization term$  \\lambda w $ ensures that the solution is unique and helps in preventing overfitting by penalizing large weights.\n",
    "\n",
    "\n",
    "## 5. Convergence to the Optimal Solution\n",
    "\n",
    "For a sufficiently small learning rate $\\eta$, the weights $\\mathbf{w}_{\\tau}$ will converge to the optimal solution $\\mathbf{w}^*$, where the gradient $\\nabla E(\\mathbf{w}) = 0$. This corresponds to the point where the model minimizes the regularized error function.\n",
    "\n",
    "The derived weight update step is crucial for implementing SGD in ridge regression, as it helps achieve a balance between fitting the data and controlling model complexity through regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa41f9a",
   "metadata": {},
   "source": [
    "# 4.2 Using the analytically derived gradient from 4.1, implement either a direct or a gradient descent algorithm for Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c2bf8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectLinearRegressorL2:\n",
    "    \"\"\"\n",
    "    Implements direct linear regression with L2 regularization.\n",
    "\n",
    "    Attributes:\n",
    "    lambda_value (float): Regularization coefficient controlling the strength of regularization.\n",
    "    coef_ (array): The fitted model parameters (i.e., regression coefficients).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lambda_value=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the DirectLinearRegressorL2 object.\n",
    "\n",
    "        Parameters:\n",
    "        lambda_value (float): Regularization coefficient, default is 0.1.\n",
    "                              When lambda_value is 0, it is equivalent to ordinary least squares linear regression.\n",
    "        \"\"\"\n",
    "        self.lambda_value = lambda_value\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        \"\"\"\n",
    "        Fits a linear model with L2 regularization based on the provided data.\n",
    "\n",
    "        Parameters:\n",
    "        x (array-like): Input feature data, shape (dataset_size, n_features).\n",
    "        y (array-like): Target values (continuous variable), shape (dataset_size).\n",
    "\n",
    "        Returns:\n",
    "        self: Returns the model object with the fitted parameters.\n",
    "        \"\"\"\n",
    "        # Compute the transpose of x multiplied by y, which is the right-hand side of the normal equation\n",
    "        rhs = x.T.dot(y)\n",
    "        \n",
    "        # Compute the transpose of x multiplied by x, then add lambda multiplied by the identity matrix, which is the left-hand side of the normal equation\n",
    "        # np.eye(x.shape[1]) creates an identity matrix with the same dimensions as the number of features in x\n",
    "        lhs = x.T.dot(x) + self.lambda_value * np.eye(x.shape[1])\n",
    "        \n",
    "        # Solve the normal equation to find the optimal parameters\n",
    "        self.coef_ = np.linalg.solve(lhs, rhs)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predicts new data points using the fitted model.\n",
    "\n",
    "        Parameters:\n",
    "        x (array-like): New input data, shape (dataset_size, n_features).\n",
    "\n",
    "        Returns:\n",
    "        array: Prediction results, shape (dataset_size).\n",
    "        \"\"\"\n",
    "        # Perform dot product on the input x and use the fitted coefficients to make predictions\n",
    "        return x.dot(self.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861a4889",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa9af011",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDLinearRegressorL2:\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent (SGD) linear regression model with L2 regularization.\n",
    "\n",
    "    Attributes:\n",
    "    batch_size (int): The number of samples used to compute the gradient during each iteration.\n",
    "    eta (float): The learning rate, which determines the step size for parameter updates.\n",
    "    tau_max (int): The maximum number of iterations.\n",
    "    epsilon (float): The convergence threshold; the algorithm stops when the parameter changes are smaller than this value.\n",
    "    random_state (int, optional): Random seed for reproducible sampling.\n",
    "    lambda_para (float): The L2 regularization parameter, controlling the strength of regularization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size=1, eta=0.01, tau_max=1000, epsilon=0.00001, random_state=36, lambda_para=2):\n",
    "        self.batch_size = batch_size\n",
    "        self.eta = eta\n",
    "        self.tau_max = tau_max\n",
    "        self.epsilon = epsilon\n",
    "        self.random_state = random_state\n",
    "        self.lambda_para = lambda_para\n",
    "        self.coef_ = None  # Stores the model coefficients\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        \"\"\"\n",
    "        Fit the SGD linear regression model.\n",
    "\n",
    "        Parameters:\n",
    "        x (np.array): Input feature data, shape (dataset_size, n_features).\n",
    "        y (np.array): Target values, shape (dataset_size).\n",
    "\n",
    "        Returns:\n",
    "        self: Returns the model object with the fitted parameters.\n",
    "        \"\"\"\n",
    "        rng = np.random.default_rng(self.random_state)\n",
    "        dataset_size, n_features = x.shape\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.w_ = np.zeros(n_features)\n",
    "        \n",
    "        # Perform iterations\n",
    "        for tau in range(1, self.tau_max + 1):\n",
    "            indices = rng.choice(dataset_size, size=self.batch_size, replace=True)\n",
    "            x_batch = x[indices]\n",
    "            y_batch = y[indices]\n",
    "            gradient = x_batch.T @ (x_batch @ self.w_ - y_batch) / self.batch_size + self.lambda_para * self.w_\n",
    "            self.w_ -= self.eta * gradient\n",
    "            \n",
    "            # Check for convergence\n",
    "            if np.linalg.norm(self.w_ - self.coef_) < self.epsilon:\n",
    "                self.coef_ = self.w_\n",
    "                break\n",
    "            \n",
    "            self.coef_ = self.w_\n",
    "        \n",
    "        return self\n",
    "                                \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict new data points using the fitted model.\n",
    "\n",
    "        Parameters:\n",
    "        x (np.array): New input data, shape (dataset_size, n_features).\n",
    "\n",
    "        Returns:\n",
    "        np.array: Prediction results, shape (dataset_size,).\n",
    "        \"\"\"\n",
    "        return x @ self.coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2fc011",
   "metadata": {},
   "source": [
    "## 4.3.a Consider the ridge regression model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c46280d",
   "metadata": {},
   "source": [
    "## Study on the Effects of L2 Regularization (Ridge Regression)\n",
    "\n",
    "We investigate the impact of L2 regularization on training and testing errors using synthetically generated data. The synthetic data generator creates data based on the following model:\n",
    "\n",
    "$$\n",
    "X \\sim \\text{Uniform}(-0.3, 0.3)\n",
    "$$\n",
    "$$\n",
    "Y = \\frac{\\sin(5 \\pi x)}{1 + 2x} + \\epsilon\n",
    "$$\n",
    "$$\n",
    "\\epsilon \\sim \\mathcal{N}(0, 0.1)\n",
    "$$\n",
    "\n",
    "### Ridge Regression with Polynomial Features\n",
    "To analyze the regularization effects, a ridge regression model is used with varying regularization strength \\( \\lambda \\). The model setup includes a polynomial feature transformer of degree 5. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae824f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"\n",
    "    Define a mathematical function with oscillatory and nonlinear characteristics.\n",
    "\n",
    "    Parameters:\n",
    "    x (array-like): Input values, typically a NumPy array.\n",
    "\n",
    "    Returns:\n",
    "    array: Computed function values.\n",
    "    \"\"\"\n",
    "    return np.sin(5 * np.pi * x) / (1 + 2 * x)\n",
    "\n",
    "def make_additive_noise_data(n, f, a, b, noise=0.1**0.5, random_state=36):\n",
    "    \"\"\"\n",
    "    Generate a dataset with additive Gaussian noise.\n",
    "\n",
    "    Parameters:\n",
    "    n (int): Number of data points to generate.\n",
    "    f (function): Function used to generate data.\n",
    "    a (float): Start of the data generation interval.\n",
    "    b (float): End of the data generation interval.\n",
    "    noise (float): Standard deviation of the additive noise, default is 0.316 (square root of 0.1).\n",
    "    random_state (int, optional): Seed for the random number generator to ensure reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two NumPy arrays, the first is the generated x values, and the second is the corresponding noisy y values.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    x = rng.uniform(a, b, size=n).reshape(-1, 1)\n",
    "    y = f(x) + rng.normal(0, noise, size=x.shape)\n",
    "    return x, y\n",
    "\n",
    "def plot_function(f, a, b, models=[], data=None, ax=None, ax_labels=True, legend=True):\n",
    "    \"\"\"\n",
    "    Plot a mathematical function and model predictions.\n",
    "\n",
    "    Parameters:\n",
    "    f (function): The mathematical function to plot.\n",
    "    a (float): Start value of the x-axis.\n",
    "    b (float): End value of the x-axis.\n",
    "    models (list of models, optional): A list of predictive models to display their predictions on the function plot.\n",
    "    data (tuple, optional): A tuple containing x and y arrays to plot as scatter points.\n",
    "    ax (matplotlib.axes.Axes, optional): The axes on which to plot.\n",
    "    ax_labels (bool, optional): Whether to add axis labels.\n",
    "    legend (bool, optional): Whether to show the legend.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    xx = np.linspace(a, b, 200).reshape(-1, 1)\n",
    "    \n",
    "    # Plot the function\n",
    "    ax.plot(xx, f(xx), color=\"black\", label='$f(x)$')\n",
    "\n",
    "    # Plot model predictions\n",
    "    for idx, model in enumerate(models):\n",
    "        predictions = model.predict(xx)\n",
    "        ax.plot(xx, predictions, label=f'Model {idx + 1}', alpha=0.5)\n",
    "\n",
    "    # Plot data points\n",
    "    if data is not None:\n",
    "        ax.scatter(data[0], data[1], color='red', alpha=0.5, label='Data points')\n",
    "\n",
    "    # Set legend and labels\n",
    "    if legend:\n",
    "        ax.legend()\n",
    "    if ax_labels:\n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('y')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# x, y = make_additive_noise_data(100, f, 0, 1)\n",
    "# plot_function(f, 0, 1, data=(x, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "192e7fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_value = [10**(-10 + 9*i/100) for i in range(0,101)]\n",
    "#lambda_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42089559",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 10000\n",
    "TEST_SIZE = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d864d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = make_additive_noise_data(TRAIN_SIZE, f, -0.3, 0.3, random_state= 36)\n",
    "x_test, y_test = make_additive_noise_data(TEST_SIZE, f, -0.3, 0.3, random_state = 36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed631eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f7a2938",
   "metadata": {},
   "outputs": [],
   "source": [
    "transormation_then_linear = [Pipeline(steps=[(\"PolyDegree\", poly), (\"ridgeRegression\", DirectLinearRegressorL2(lambda_value=each))]) for each in lambda_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c7329f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;PolyDegree&#x27;, PolynomialFeatures(degree=5)),\n",
       "                (&#x27;ridgeRegression&#x27;,\n",
       "                 &lt;__main__.DirectLinearRegressorL2 object at 0x000001EBD08279A0&gt;)])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;PolyDegree&#x27;, PolynomialFeatures(degree=5)),\n",
       "                (&#x27;ridgeRegression&#x27;,\n",
       "                 &lt;__main__.DirectLinearRegressorL2 object at 0x000001EBD08279A0&gt;)])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PolynomialFeatures</label><div class=\"sk-toggleable__content\"><pre>PolynomialFeatures(degree=5)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DirectLinearRegressorL2</label><div class=\"sk-toggleable__content\"><pre>&lt;__main__.DirectLinearRegressorL2 object at 0x000001EBD08279A0&gt;</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('PolyDegree', PolynomialFeatures(degree=5)),\n",
       "                ('ridgeRegression',\n",
       "                 <__main__.DirectLinearRegressorL2 object at 0x000001EBD08279A0>)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transormation_then_linear[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df920b9",
   "metadata": {},
   "source": [
    "# 4.3.b Fit each model at least ten times  for each repetition all models use the same training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91695268",
   "metadata": {},
   "source": [
    "## Study Design and Mathematical Formulation\n",
    "\n",
    "### Objective\n",
    "The objective is to evaluate the impact of L2 regularization on model performance using the mean squared error (MSE) metric.\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "The MSE is calculated for each model configuration as follows:\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "where \\(n\\) is the number of samples, \\(y_i\\) is the true value, and \\(\\hat{y}_i\\) is the predicted value by the model.\n",
    "\n",
    "### Experimental Setup\n",
    "- **Resampling Strategy**: For each lambda value, fit each model at least ten times using resampled training datasets of size 20.\n",
    "- **Consistency**: To ensure consistency and reduce variance in the experiment:\n",
    "  - Perform model training for each lambda as an inner loop.\n",
    "  - Resample the training dataset once per outer loop iteration and use this dataset across all lambda configurations.\n",
    "\n",
    "### Regularization Parameter Range\n",
    "Lambda values are tested over a logarithmic scale:\n",
    "$$\n",
    "\\lambda \\in \\{10^{-10 + 9i/100} : 0 \\leq i \\leq 100\\}\n",
    "$$\n",
    "This provides a comprehensive view over a range from very light to relatively strong regularization effects.\n",
    "\n",
    "### Implementation Note\n",
    "Ensure that all models use the same randomly resampled training dataset for each of the ten repetitions to maintain experimental control and reliability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "697d40da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(lambda_value, pipelines):\n",
    "    \"\"\"\n",
    "    Run multiple data processing pipelines and evaluate their performance.\n",
    "\n",
    "    Parameters:\n",
    "    lambda_value (float): Regularization parameter, used to adjust the strength of regularization.\n",
    "    pipelines (list): A list containing multiple preprocessing and learning pipelines.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Two lists containing the average training and testing mean squared errors for each pipeline.\n",
    "    \"\"\"\n",
    "    l2_mse_train = []  # Store the average training mean squared error for each pipeline\n",
    "    l2_mse_test = []   # Store the average testing mean squared error for each pipeline\n",
    "    min_mse = None     # Initialize the minimum mean squared error\n",
    "    best_l2 = None     # Best regularization parameter value\n",
    "    best_model = None  # Best model\n",
    "\n",
    "    # Iterate through all pipelines\n",
    "    for eachpipe in pipelines:\n",
    "        mse_train = 0\n",
    "        mse_test = 0\n",
    "        # Repeat the experiment 10 times to stabilize the performance evaluation\n",
    "        for i in range(1, 11):\n",
    "            # Stack the features and targets of the training data together\n",
    "            xAndy = np.hstack((x_train, y_train.reshape(y_train.shape[0], 1)))\n",
    "            # Randomly resample 20 samples from the stacked data\n",
    "            sampled_data = resample(xAndy, n_samples=20, random_state=36)\n",
    "            x_train_sampled = sampled_data[:, :-1]\n",
    "            y_train_sampled = sampled_data[:, -1:]\n",
    "            # Train the model on the resampled data\n",
    "            model = eachpipe.fit(x_train_sampled, y_train_sampled)\n",
    "            # Predict on the test set\n",
    "            y_test_pred = model.predict(x_test)\n",
    "            # Predict on the training set samples\n",
    "            y_train_pred = model.predict(x_train_sampled)\n",
    "            # Accumulate mean squared error\n",
    "            mse_test += np.sum(mean_squared_error(y_test, y_test_pred))\n",
    "            mse_train += np.sum(mean_squared_error(y_train_sampled, y_train_pred))\n",
    "        \n",
    "        # Compute the average mean squared error over the 10 experiments\n",
    "        mse_test /= 10\n",
    "        mse_train /= 10\n",
    "        l2_mse_train.append(mse_train)\n",
    "        l2_mse_test.append(mse_test)\n",
    "\n",
    "    return l2_mse_train, l2_mse_test\n",
    "\n",
    "# Example usage\n",
    "l2_mse_train, l2_mse_test = run(lambda_value, transormation_then_linear)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4e69e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(lambda_value, repetitions, pipelines):\n",
    "    l2_mse_train = []\n",
    "    l2_mse_test = []\n",
    "\n",
    "    for i in range(repetitions):\n",
    "        # Resample training data for each repetition\n",
    "        xAndy = np.hstack((x_train, y_train.reshape(y_train.shape[0], 1)))\n",
    "        sampled_data = resample(xAndy, n_samples=20, random_state=36)\n",
    "        x_train_sampled = sampled_data[:, :-1]\n",
    "        y_train_sampled = sampled_data[:, -1:]\n",
    "\n",
    "        for lambda_idx, pipeline in enumerate(pipelines):\n",
    "            pipeline.fit(x_train_sampled, y_train_sampled)\n",
    "            y_test_pred = pipeline.predict(x_test)\n",
    "            y_train_pred = pipeline.predict(x_train_sampled)\n",
    "\n",
    "            if len(l2_mse_test) <= lambda_idx:\n",
    "                l2_mse_test.append(np.sum(mean_squared_error(y_test, y_test_pred)))\n",
    "                l2_mse_train.append(np.sum(mean_squared_error(y_train_sampled, y_train_pred)))\n",
    "            else:\n",
    "                l2_mse_test[lambda_idx] += np.sum(mean_squared_error(y_test, y_test_pred))\n",
    "                l2_mse_train[lambda_idx] += np.sum(mean_squared_error(y_train_sampled, y_train_pred))\n",
    "\n",
    "    # Calculate average MSE across repetitions\n",
    "    l2_mse_test = [mse / repetitions for mse in l2_mse_test]\n",
    "    l2_mse_train = [mse / repetitions for mse in l2_mse_train]\n",
    "\n",
    "    return l2_mse_train, l2_mse_test\n",
    "\n",
    "# 定义重复实验的次数\n",
    "repetitions = 10\n",
    "\n",
    "\n",
    "# 调用run_experiment函数\n",
    "l2_mse_train, l2_mse_test = run_experiment(lambda_value, repetitions, transormation_then_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065b0999",
   "metadata": {},
   "source": [
    "# 4.3.c Create a plot of mean squared errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87d9e37e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1ebd0846590>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAG1CAYAAAAV2Js8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABa9UlEQVR4nO3dd3zM9+MH8Ndd9p4iOxIjRGKFmCW0xEqL1teovYuqqqKDDlrt92sT1WhLjdYWW8SerSAIIUhCQhJZsmXdfX5/pO7XNKjjLp8br+fjcY/Lfe7u836dT5t75TMlgiAIICIiItJDUrEDEBEREYmFRYiIiIj0FosQERER6S0WISIiItJbLEJERESkt1iEiIiISG+xCBEREZHeYhEiIiIivWUodgBNJpfLkZqaCisrK0gkErHjEBER0QsQBAEFBQVwdXWFVPr8dT4sQs+RmpoKDw8PsWMQERHRS0hJSYG7u/tzX8Mi9BxWVlYAKv8hra2tRU5DRERELyI/Px8eHh6K7/HnYRF6jiebw6ytrVmEiIiItMyL7NbCnaWJiIhIb7EIERERkd7ipjEVkMlkKC8vFzuG1jIyMoKBgYHYMYiISA/pfBHau3cvPvroI8jlcsycORNjxoxR2bwFQUB6ejpyc3NVNk99ZWtrC2dnZ56mgIiIapROF6GKigpMmzYNx44dg7W1NVq0aIF+/frB3t5eJfN/UoKcnJxgbm7OL/GXIAgCiouLkZGRAQBwcXEROREREekTnS5C58+fR+PGjeHm5gYA6NmzJyIjIzFo0KBXnrdMJlOUIAcHh1eenz4zMzMDAGRkZMDJyYmbyYiIqMZo9M7SJ0+eRGhoKFxdXSGRSBAREVHtNStXroS3tzdMTU0RGBiIU6dOKZ5LTU1VlCAAcHd3x4MHD1SS7ck+Qebm5iqZn7578u/Ifa2IiKgmaXQRKioqQtOmTbFixYqnPr9582ZMnToVn332GWJiYvDaa6+hR48eSE5OBlC52eWfnrf5qrS0FPn5+VVu/4abw1SD/45ERCQGjS5CPXr0wLx589CvX7+nPr9o0SKMHj0aY8aMQaNGjbBkyRJ4eHjghx9+AAC4ublVWQN0//795+6DMn/+fNjY2ChuvLwGERGRbtPoIvQ8ZWVluHjxIrp161Zlerdu3XD27FkAQFBQEK5du4YHDx6goKAA+/fvR0hIyDPn+cknnyAvL09xS0lJUetnICIiInFp7c7SWVlZkMlkqF27dpXptWvXRnp6OgDA0NAQCxcuROfOnSGXyzFjxozn7thsYmICExMTtebWVcHBwWjWrBmWLFkidhQiIqIXprVF6Il/7lsiCEKVaW+++SbefPPNmo6lsf5tX5zhw4dj7dq1Ss93x44dMDIyeslURERE4tDaIuTo6AgDAwPF2p8nMjIyqq0lov+Xlpam+Hnz5s2YM2cO4uPjFdOeHMr+RHl5+QsVHFWdm4mIiPSDTC7DsbvHYGpoig6eHUTLobX7CBkbGyMwMBBRUVFVpkdFRaFdu3YipdJ8zs7OipuNjQ0kEonicUlJCWxtbbFlyxYEBwfD1NQUGzZsQHZ2NgYNGgR3d3eYm5sjICAAv//+e5X5BgcHY+rUqYrHderUwbfffotRo0bBysoKnp6eCA8Pr+FPS0REmiYuMw6zDs+C1xIvdF3fFT/H/CxqHo1eI1RYWIg7d+4oHiclJeHy5cuwt7eHp6cnpk2bhqFDh6Jly5Zo27YtwsPDkZycjAkTJoiWubi8GDezbtb4uA0dG8LcSDXnNJo5cyYWLlyINWvWwMTEBCUlJQgMDMTMmTNhbW2Nffv2YejQofDx8UHr1q2fOZ+FCxdi7ty5+PTTT7Ft2za899576NixIxo2bKiSnEREpB2KyorwW+xvCL8UjgupF2BnaodB/oMwrOkwBLkFiZpNo4vQhQsX0LlzZ8XjadOmAfj//VgGDBiA7OxsfP3110hLS4O/vz/2798PLy8vsSLjZtZNBIYH1vi4F8ddRAuXFiqZ19SpU6udsmD69OmKn99//30cPHgQW7dufW4R6tmzJyZOnAigslwtXrwYx48fZxEiItITN7Nu4ofoH7D2yloUlhWiZ/2e2PGfHehZvydMDDXj4CSNLkLBwcFPPSni302cOFHxZasJGjo2xMVxF0UZV1VatmxZ5bFMJsN3332HzZs348GDBygtLUVpaSksLCyeO58mTZoofn6yCe7JNcWIiEg3CYKAI0lH8P2Z73E48TBqmdfCpFaTMD5wPLxsxVtR8SwaXYS0kbmRucrWzIjlnwVn4cKFWLx4MZYsWYKAgABYWFhg6tSpKCsre+58/rmTtUQigVwuV3leIiISn0wuw44bO/Ddme9wKe0Smjs3x4a+G/CO3zsas/bnaViEniIsLAxhYWGQyWRiR9EIp06dwltvvYUhQ4YAAORyOW7fvo1GjRqJnIyIiMQmk8uw4eoGzDs1D3dy7uB179dxaMghvOHzhlZcPklrjxpTp0mTJiEuLg7R0dFiR9EI9erVQ1RUFM6ePYsbN25g/Pjx1U5bQERE+kUuyLHl+hb4/+CPEbtGIMApANFjo3F42GF0rdtVK0oQwDVC9AJmz56NpKQkhISEwNzcHOPGjUOfPn2Ql5cndjQiIqphgiDgwJ0D+PTIp7jy8Aq61+uODX03INC15g8UUgWJ8G97I+ux/Px82NjYIC8vD9bW1lWeKykpQVJSEry9vWFqaipSQt3Bf08iIs13LeMapkVOQ1RiFDp6dcQ3Xb556ZMhCnI5SnJyAEGAWa1aKs35vO/vf+IaISIiInqujKIMzDk2B6svrUZdu7rYNXAXQhuEPnfzlyAIKM3JQUFKCgqTkyvv799HcVoaitPTUZyeDnl5OXz69kWbefNq8NNUxSJERERET1Uhr0DY+TDMOT4HUokUC7ouwKSgSTA2MFa8RhAEPM7MRG58PPISEpCfmIi8hATkJSSgvKBA8TpTR0dYeXjA3MUFDgEBMHd2hoWLC6x9fMT4aAosQkRERFTNH/f/wHv73sOV9CsYHzgec7vMhb2xLfITEnE/Lg6P4uOR+9etNDcXAGBgZgZrb2/Y1K0Lt+BgWHl5wcrLC5bu7jD6l3PPiYVFiIiIiBRyHufgk8Of4Jfo1ehq3Az/c/0e1pce4+KGyci9dQuy0lIAgKWHB+x8fdHg3Xdh6+sLuwYNYOHmBolUuw5IZxEiIiLScxWPH+NRfDwOH92IM6e2wzvLAGvzGkMiK0WmdB1KfXxg5+cHr549Ye/nB1tfXxhbWYkdWyVYhIiIiPSEIAgoTktD7u3byL11q/IWH4+8pCRALkeFRICfky0atg2GW9NA2DduDDtfXxiamYkdXW1YhIiIiHSMrKwMhffvo+DuXeQnJVXuwJyYiPzERJQXFgIAjCwtYdOgATLrWGBLrSxkOhlg1sCFGNZ0gMjpaxaL0FPwEhtERKTJ5DIZSjIzUfjgAYrS0lD04AGKHjxAYUoKClJSUJyeDvx1mkBDC4vKHZh9fODepQts69eHbYMGyDQrw5g9Y3Ak6QhGvzEaG7v+D3ZmdiJ/sprHIvQUkyZNwqRJkxQnZCIiIqpJsrKyKgWnKDUVRWlpKE5Lq7x/+BBCRYXi9Sa2tjB3dYWVhwccmjSBlacnLD09YeXpCTMnpyrn+xEEAT9d+gnTDk2DnakdIodEolvdbmJ8TI3AIqRn/u3aL8OHD8fatWtfat516tTB1KlTMXXq1Jd6PxGRvinLz8ejGzcqz7/z12asgrt3UZSWplijI5FKYebkBAtXV5i7uMCxWTNYuLjA3MUFFm5usHBxeeFD01PyUjBmzxgcSjiEMc3HYEG3BbAx1e8/+FmE9ExaWpri582bN2POnDmIj49XTDPT4R3iiIjEJJfJkHP9OjIvXkTO9evIvn4dhcnJAACpoSEsPT1h7e0Nrx49FOfesXBzg3nt2pAaGb3S2IIgYN2VdZhycAosjS2xf/B+9KjfQxUfS+uxCOkZZ2dnxc82NjaQSCRVpu3Zswdffvklrl+/DldXVwwfPhyfffYZDA0r/1P58ssv8csvv+Dhw4dwcHDAO++8g2XLliE4OBj37t3Dhx9+iA8//BBA5f94RET6rCg1FamnTyP97Fk8/PNPlOXnw8DMDPYNG8K1Y0fY+/nBvnFjWNepA6mher6SM4oyMH7veETcjMCQJkOwrPsyvdwX6FlYhEghMjISQ4YMwbJly/Daa68hISEB48aNAwB88cUX2LZtGxYvXoxNmzahcePGSE9Px5UrVwAAO3bsQNOmTTFu3DiMHTtWzI9BRCSq0txcJEdG4u6ePciMiYHEwAAOAQFoMGQIXNq2hUNAwCuv4XlRETcjMG7POAgQsP0/29GvUb8aGVebsAipWMXjx8hPSqrxca29vV/5PA/ffPMNZs2aheHDhwMAfHx8MHfuXMyYMQNffPEFkpOT4ezsjDfeeANGRkbw9PREUFAQAMDe3h4GBgawsrKqsoaJiEgfCIKAh3/8gVu//47UEycgyOVwbtcObb//Hm6dOtX4yQfzSvIwNXIq1l5eizd930R473DUtqxdoxm0BYuQiuUnJeFg//41Pm73rVth7+f3SvO4ePEioqOj8c033yimyWQylJSUoLi4GP3798eSJUvg4+OD7t27o2fPnggNDVVsNiMi0jdymQz3jxxB3E8/Ief6ddj6+qLZ9Onw6tEDZo6OomQ6lnQMI3aNwKPHj/DLm79gRLMR/3qgjD7jN5iKWXt7o/vWraKM+6rkcjm++uor9OtXfdWpqakpPDw8EB8fj6ioKBw+fBgTJ07E//73P5w4cQJGNbSal4hIEwhyOZJ270bcTz8hPykJtYOC0Hn1aji3bSta6Xhc/hifHvkUS/5cguA6wVg7Yi28bL1EyaJNWIRUzNDM7JXXzIilRYsWiI+PR7169Z75GjMzM7z55pt48803MWnSJDRs2BCxsbFo0aIFjI2NeRJKItJ5OdevI3ruXGTHxsK9Sxe0+fZbODZpImqm6AfRGBYxDEmPkrA4ZDGmtJ4CqUS7Ln4qFhYhUpgzZw569+4NDw8P9O/fH1KpFFevXkVsbCzmzZuHtWvXQiaToXXr1jA3N8f69ethZmYGL6/Kvzjq1KmDkydPYuDAgTAxMYGjSKuFiYjUoTQ3F1eWLcOdLVtgW78+3li3Dk6BgaJmKpeVY+7Jufj21Ldo5twMl8Zfgl8t7fxjXCysi6QQEhKCvXv3IioqCq1atUKbNm2waNEiRdGxtbXF6tWr0b59ezRp0gRHjhzBnj174ODgAAD4+uuvcffuXdStWxe1atUS86MQEalUcmQk9vbqhXv79qHFzJnovnWr6CXoesZ1tPm5Deafno/ZHWfj3OhzLEEvQSLwZC/V/P1aY7du3UJeXh6sra2rvKakpARJSUnw9vaGqampSEl1B/89iUgTyUpLcfH773Fn82Z4hoQg8JNPYCbyH3oyuQyLzi3C7GOzUde+Ltb1WYdAV3FLmaZ5comsp31//xM3jT0FrzVGRET5d+/i9LRpyE9KQtAXX6Bu//6iH311J+cOhkcMx7mUc5jWdhrmdZkHU0P+8fgqWISIiIj+4e7+/Tj/xRcwc3JCyO+/w65hQ1HzyAU5VkavxMzDM+Fs6YyTI0+ig2cHUTPpChYhIiKivwiCgLjVq3Fl6VJ49eqFoC++eOELmqrLvdx7GLV7FI4mHcXElhPxfdfvYWlsKWomXcIiREREhMpzA1387jvc2rgRAZMmwf+990TdFCYIAn6O+RnTIqfB1tQWUUOj8IbPG6Ll0VUsQkREpPdkZWU49+mnSImMRKs5c1B/wABR86QWpGLM7jE4cOcARjYbicUhi2Fjyn1W1YFF6BXxoDvV4L8jEYmlvKgIJ6dMQebFi+iwaBE8unYVLYsgCPj92u+YtH8STA1NsWfQHvRu0Fu0PPqAReglPbmkRHFxMcxe8WKnVPnvCICX6iCiGlVRUoITEyfi0c2b6Bwejtp/XUhaDNnF2Xhv33vYGrcVA/0HYkWPFXAwdxAtj75gEXpJBgYGsLW1RUZGBgDA3Nxc9MMqtZEgCCguLkZGRgZsbW1hYGAgdiQi0hPy8nKcnjYN2devo8vq1ajVvLloWfbd2ocxe8agTFaGTW9vwgB/cTfN6RMWoVfg7OwMAIoyRC/P1tZW8e9JRKRuglyOc599hvQzZ9AxLEy0ElRUVoRpkdMQfikcPer1wE9v/gRXK1dRsugrFqFXIJFI4OLiAicnJ5SXl4sdR2sZGRlxTRAR1RhBEHDh22+RfOAA2i9YANcO4pyP52LqRQzeMRj38+9jVa9VGBc4jlsWRMAipAIGBgb8Iici0hKxK1bg9u+/I+irr+AZElLj48sFORacXYDPj36OgNoBuDTuEnwdfWs8B1ViESIiIr2RtHcvrq1ahWYffoh677xT4+OnFqRi2M5hOJp0FB+3+xhzu8yFsYFxjeeg/8ciREREeiEnLg7n58yB91tvodHo0TU+/rGkYxi4fSAMJAaIGhqF131er/EMVJ1U7ACaKCwsDH5+fmjVqpXYUYiISAVKcnJwcsoU2NSrh6AvvqjRfXHkghzfnvoWb6x/AwFOAbg84TJLkAaRCDyT3TM9ufp8Xl4erK2txY5DREQvQV5ejqNjxyI/MREhmzfDwsWlxsbOeZyDYTuHYd/tfZjdcTa+6PQFDKTcp1TdlPn+5qYxIiLSaZcWLEBmTAxe/+WXGi1B1zKuIfT3UOSX5mP/4P3oUb9HjY1NL46bxoiISGcl7dmDWxs2IHDWLDgFBtbYuAduH0C7n9vB2sQal8ZdYgnSYCxCRESkkwpTUhD99deoExqK+gMH1siYgiBg2Z/L0Pv33giuE4zTI0/Dy9arRsaml8MiREREOkdeUYGzs2bBxN4erT7/vEZ2ji6XlWPS/kn44OAH+LDNh9g5YCesTKzUPi69Gu4jREREOud6eDiyr17FG+vWwcjSUu3jlVSU4D9b/4MDdw5gdehqjGkxRu1jkmqwCBERkU7JunIF11atQuPx42vkGmKFZYXos6kPzqScwZ5Be9C9Xne1j0mqwyJEREQ6o7yoCGdnzoS9vz/8J0xQ+3i5JbnoubEnrmVcQ+SQSHT06qj2MUm1WISIiEhnXJw/HyXZ2egcHg6poXq/4jKLMtFtQzfcy72HI8OOoJUbT8KrjViEiIhIJ6SeOoXEnTvRet48WHl6qnWsrOIsdFrbCTmPc3BixAkE1A5Q63ikPixCRESk9cqLinD+q6/g3K4dfPr0UetYBaUF6LmxJ7KKs3Bq5CleOV7LsQgREZHWu7JsGUpzc9V+HbHSilL03dwX8dnxOD78OEuQDuB5hIiISKtlXbmCWxs3oun778PS3V1t48jkMry7412cTj6N3QN3o7mL+o9II/XjGiEiItJasrIy/DlnDuwbN0aDIUPUNo4gCHhv33uIuBmBHQN2oFOdTmobi2oWixAREWmtuJ9/Rv7du+i+eTOkBuq7qvs3p77B6kurseatNXjT9021jUM1j5vGiIhIK+UlJOD6jz/Cb9Qo2DVsqLZx9t7aiznH5uDLTl9iRLMRahuHxMEi9BRhYWHw8/NDq1Y8JwQRkSYSBAHRc+fCwtVVrSdOvJV9C+/ueBehvqGY3Wm22sYh8UgEQRDEDqGp8vPzYWNjg7y8PFhbW4sdh4iI/nLvwAGcmT4dwT/+CNcOHdQyRkFpAdr83AYyuQx/jvkTNqY2ahmHVE+Z72/uI0RERFqlvKgIl/73P7i/8YbaSpAgCBixawRS8lJwfux5liAdxiJERERa5dqqVSjLzUWLGTPUNsb80/Ox48YORAyIQENH9e1/ROLjPkJERKQ18hITEb9uHfzGjoWlm5taxjibchazj83G5699jrcavqWWMUhzsAgREZFWEAQBF+fPh7mLC/xGjVLLGIVlhRi2cxiC3ILwRfAXahmDNAs3jRERkVa4f/gw0s+eRaewMBiYmKhljBlRM5BWmIYD7x6AoZRfkfqAS5mIiDRexePHuPj993Dt1AluwcFqGSPyTiR+uPADwnqGob5DfbWMQZqHRYiIiDTejbVrUZKZicCff1bL/B89foRRu0ehq09XvNfyPbWMQZqJ+wgREZFGK87IQNzPP8N36FBYeXmpZYzJByajqKwIv7z1i1qvXk+ah2uEiIhIo11ZsgSGZmZoPH68Wua/PW47fov9DRv6boC7tfquXk+aiWuEiIhIY2Vfu4akXbvQ5P33YWxlpfL5F5UVYWrkVIQ2CMXggMEqnz9pPhYhIiLSSIIg4NJ338Gmfn3U7ddPLWN8e+pbZBZlYmn3pdwkpqe4aYyIiDRScmQkMmNi0Hn1akgNVf91dTv7NhacW4BZ7WfB285b5fMn7cA1QkREpHEqSkpweeFCuAUHw6VdO5XPXxAEfHDwA7hYumBmh5kqnz9pD64RIiIijRO/fj2KMzLQefVqtcx/7629OHDnAHb8ZwfMjczVMgZpB64RIiIijfI4KwvXw8PRYOBAWNepo/L5l1SUYGrkVHSr2w19GvZR+fxJu3CNEBERaZTYFSsgNTSE/3vqObHhgrMLkJKXgv2D93MHaeIaISIi0hy5t28jYft2+L/3HkxsbVU+//TCdHx76ltMbTMVvo6+Kp8/aR8WISIi0hgx//sfLNzdUX/gQLXMf/6p+TAxNMGnr32qlvmT9mERIiIijZB66hTSzpxB8+nTYWBsrPL5p+SlYNXFVZjedjpsTW1VPn/STixCTxEWFgY/Pz+0atVK7ChERHpBXlGBmP/9D06tWsG9Sxe1jPHNqW9gZWyFKa2nqGX+pJ1YhJ5i0qRJiIuLQ3R0tNhRiIj0QsL27chLTESLGTPUsgNz4qNE/BzzM2a2nwkrE9VfqoO0F4sQERGJqqygAFdXrIB3aCjs/fzUMsbck3PhYOaASUGT1DJ/0l48fJ6IiER1/ccfUfH4MZp+8IFa5h+fFY91V9ZhcchinjyRquEaISIiEk3BvXuIX78efqNHw9zZWS1jfHXiK7hauWJc4Di1zJ+0G9cIERGRaGIWLICpoyMajRihlvlfy7iGTdc24YdeP8DU0FQtY5B24xohIiISRfoff+D+0aNo9tFHMDQzU8sY80/Ph6eNJ0Y2H6mW+ZP2YxEiIqIaJ6+owMXvvoNjs2bw6tFDLWOk5KVg87XN+LDNhzA2UP15iUg3cNMYERHVuITt25F3+zZCNm1S2/W+lv25DJbGlhjVfJRa5k+6gWuEiIioRpXl5+Pq8uXwfustOAQEqGWM/NJ8hF8Kx/jA8TxvED0XixAREdWo2JUrISspQdOpU9U2xs+XfkZxeTHeb/2+2sYg3cAiRERENebRzZu49dtv8J8wAeZOTmoZo0JegaV/LsVA/4Fwt3ZXyxikO7iPEBER1QhBLkf03Lmw8vKC77Bhahtne9x23Mu7h4i2EWobg3QHixAREdWIxF27kHX5Ml7/5Re1XF0eAARBwMJzC9HFuwuaOTdTyxikW1iEiIhI7Upzc3F54UJ49eqF2q1bq22c08mnEZ0ajX2D96ltDNIt3EeIiIjU7sqyZZCXl6PFxx+rdZyF5xaioWNDdK/XXa3jkO5gESIiIrXKjo3FnS1b0OT992FWq5baxrmXew+743fjwzYfQirh1xu9GP6XQkREaiOXyRA9dy7sfH1Rf+BAtY71c8zPsDC2wOCAwWodh3QL9xEiIiK1ufnrr8iJi0O3jRshNVTfV06FvAK/xPyCdwPehaWxpdrGId3DNUJERKQWeYmJuLp8ORoOHw7Hpk3VOtaB2wfwoOABxgWOU+s4pHtYhIiISOXkMhn++PxzWLi6osn76j+78+pLq9HCpQVauLRQ+1ikW5QqQuXl5Rg5ciQSExPVlYeIiHTAzV9/RfbVq2gzbx4MTU3VOtb9/PvYd3sfxrXg2iBSnlJFyMjICDt37lRXFiIi0gGKTWLDhqFW8+ZqH29NzBqYGppiUMAgtY9FukfpTWN9+/ZFRESEGqIQEZG2q7JJbMoUtY8nk8vwU8xPGOQ/CNYm1mofj3SP0rvw16tXD3PnzsXZs2cRGBgICwuLKs9PqYH/8ImISDPd+OUXZF+9iq7r1ql9kxgARCVGITkvGWNbjFX7WKSbJIIgCMq8wdvb+9kzk0h0av+h/Px82NjYIC8vD9bW/EuDiOh5Mi5exJGRI9Fo5Eg0+/DDGhmz3+Z+SHiUgMvjL0MikdTImKT5lPn+VnqNUFJS0ksHIyIi3VSSnY0z06ejVvPmNXKUGACkFaRhz609WByymCWIXtorHT4vCAKUXKGkFcLCwuDn54dWrVqJHYWISOPJZTKcmTEDgkyG9gsWqPXEiX+37so6GEoN8W7AuzUyHummlypC69atQ0BAAMzMzGBmZoYmTZpg/fr1qs4mmkmTJiEuLg7R0dFiRyEi0njXfvgBGefPo91//6vWa4n904bYDejTsA/szOxqbEzSPUrX9kWLFmH27NmYPHky2rdvD0EQcObMGUyYMAFZWVn4sIa2CxMRkfjSzpzBtVWr0GTyZDi3aVNj4159eBXXMq5h/uvza2xM0k1KF6Hly5fjhx9+wLBhwxTT3nrrLTRu3BhffvklixARkZ4ouHcPZ2fOhEv79mg8rmZPZrjx6kY4mDkgpG5IjY5LukfpTWNpaWlo165dtent2rVDWlqaSkIREZFme5yZiWPjx8PE1hbtvv8eEmnNXbFJLsjx+7Xf0d+vP4wMjGpsXNJNSv+XW69ePWzZsqXa9M2bN6N+/foqCUVERJqrvKgIx997D7KSEnQOD4eJrW2Njn86+TRS8lPwbhPuJE2vTulNY1999RUGDBiAkydPon379pBIJDh9+jSOHDny1IJERES6Q1ZWhlMffIDClBS8sW4dLFxdazzDxqsb4WXjhXYe1bdOEClL6TVCb7/9Ns6fPw9HR0dERERgx44dcHR0xPnz59G3b191ZCQiIg0gyOX447PPkHHhAjouXw47X98az1AmK8PWuK0Y5D8IUknNbY4j3aXUGqHy8nKMGzcOs2fPxoYNG9SViYiINIxcJsOFuXNx78ABdFi4ELWDgkTJcfDOQTwqecTNYqQyvPo8ERE9l6y0FGemTUPCjh1oM3cuPEPEO1JrY+xGNKndBP5O/qJlIN3Cq88TEdEzleXn49i4cUg9dQqvLV0KHxF3gcgvzcfu+N08kzSpFK8+T0RET1WckYHj48ejOD0dXX7+GbWaNxc1T8TNCJRUlGCg/0BRc5Bu4dXnn4NXnycifZV19SrOfPQRBLkcnX/8ETb16okdCSEbQlBSUYITI06IHYU0nNquPi8IAo4dOwYnJyeYm5u/UkgiItI8cpkMcT/9hNiwMNg3bowOixbBwsVF7FjILMrE4cTDWNlzpdhRSMcotY+QIAho0KABHjx4oK48REQkkqLUVBwdNQpXly+H35gx6LpunUaUIADYFb8LANC3EU/TQqql1BohqVSK+vXrIzs7m2eRJiLSEYJcjqRdu3Dxv/+Fkbk53li7Fk4tW4odq4odN3bgNc/X4GThJHYU0jFKHzX23//+Fx9//DGuXbumjjxERFSD0v/4Awf798cfn38O1w4d0HPHDo0rQXkleTiceBj9GvUTOwrpIKWPGhsyZAiKi4vRtGlTGBsbw8zMrMrzOTk5KgtHRETqkXvnDi4vXIjUkyfh0LQpum7YIPpRYc+y99ZelMvL0bchN4uR6ildhJYsWaKGGEREpG6CXI60M2dw6/ffkXryJCzd3dFh0SJ4dOsGiUQidrxn2nFzB4LcguBh4yF2FNJBSheh4cOHqyMHERGpSVleHhJ378bt339Hwb17sGvYEK2/+gp1QkNhYGwsdrznKiorwoHbB/Bl8JdiRyEdpXQRAoCEhASsWbMGCQkJWLp0KZycnHDw4EF4eHigcePGqs5IRERKepyVhftHjyIlKgoPz58HAHh27Yo28+bBsXlzjV4D9HeRCZF4XPGYm8VIbZQuQidOnECPHj3Qvn17nDx5Et988w2cnJxw9epV/PTTT9i2bZs6chIR0XNUlJQg+8oVPLxwAQ//+AOZMTGQSKVwatkSgTNnwqNrV5jVqiV2TKVtv7EdAU4BqO/AI5VJPZQuQrNmzcK8efMwbdo0WFlZKaZ37twZS5cuVWk4IiKqTpDLUZCSgkdxcciJi0PWlSvIvnoV8vJyGNvYwKllS7T++mu4de4MUzs7seO+tNKKUuy9tRfT2kwTOwrpMKWLUGxsLH777bdq02vVqoXs7GyVhCIiosqT2BanpyM/MRF5iYmV9wkJeHTzJiqKigAA5s7OcAgIQPOPP0btVq1gU68eJFKlz4yikY4kHUF+aT4Pmye1UroI2draIi0trdo1x2JiYuDm5qayYERE+kBWWoqi1FQU3r+PogcPUJCSgsKUFBQkJ6Pw/n3IHj8GAEiNjWFdpw6s69aFW8eOsPPzg12jRlq9xuff7LixA/Xt68PfyV/sKKTDlC5CgwcPxsyZM7F161ZIJBLI5XKcOXMG06dPx7Bhw9SRkYhIKwmCgPL8fBSlpVXeUlNR/OTntDQUPXiAkqwsxeslBgawcHWFlacnnAIDUbdvX1h5ecHaxwcWbm6QGhiI+GlqVoW8Arvid2F089Fas2M3aSeli9A333yDESNGwM3NDYIgwM/PDzKZDIMHD8bnn3+ujoxERBpJVlqKovR0FKeloTg9HUV/3f/9cUVxseL1UiMjmDs7w8LVFTbe3nBp3x6Wbm6wcHODpZsbzJycIDV8qYN5dc6pe6eQVZyFtxu9LXYU0nESQRCEl3ljYmIiLl26BLlcjubNm+vktcfy8/NhY2ODvLw8WFtbix2HiGqQvKICjzMzK4vNX7eifxSe0n+cSd/UwQHmzs4wd3GpLDwuLrBwcYH5X/emDg46s/+Oun1w4APsuLkDyVOTuUaIlKbM9/dL/+nh4+MDHx+fl307EZFoBLkcJTk5VUpOcXp6lbU7jzMzIchkivcYmptXlhpnZ9g3agT3Ll0Uj81dXGDh7AwDExMRP5XuEAQBe27tQWiDUJYgUjuugyUinSIIAsry8qpuqvpH2Xn88CHk5eWK90iNjSvX4Dg7w8rTE7WDgv5/rY6zM8ydnWFkZcUv5RpyI+sGknKT0LtBb7GjkB5gESIirSIvL0fxw4coSk2tvKWlKXZAflJ+nhxpBQASQ0OYOzlVrrlxdoZDkyaKzVZPppnY2bHkaJC9t/bC3MgcXby7iB2F9ACLEBFpFEEQUJKdjcKUFBTev6+4f3KI+eOHDyHI5YrXP9kvx8LFBS7t2/9/yfmr6Jg6OOjV0Va6YO+tvXjD5w2YGpqKHYX0AIsQEYmivKgI+YmJyL97FwV37yL/3j0U3L2Lgnv3qhxpZergAAt3d1i6ucGxWbPKo6xcXWHh6gpzFxcYmvLLUpfkPM7BmZQzWNVrldhRSE+8UBG6evXqC8+wSZMmLx2GiHSPvLwceYmJeHTzJnJv3UJeQgLy7txBcVqa4jVmTk6w8vKCg78/6vTqBUsPj8qbuzuMLCxETE817eCdg5ALcvSs31PsKKQnXqgINWvWDBKJBIIg/Ot2dNnfjrIgIv0iLy9H7u3byI6NRfa1a3h04wby7txR7Jhs4eYGm3r14NWjB2zq1YNN3bqw9vZm2SGFPbf2oIVLC7hZ80oFVDNeqAglJSUpfo6JicH06dPx8ccfo23btgCAc+fOYeHChfjvf/+rnpREpJFKcnKQGRODzIsXkXn5Mh7duAF5WRkkhoawrV8f9n5+8OnbF3YNG8LO1xdGlpZiRyYNVi4rx8E7BzElaIrYUUiPvFAR8vLyUvzcv39/LFu2DD17/v9qyyZNmsDDwwOzZ89Gnz59VB6SiDRDSU4OHp4/j/Rz55B58SLy//ojydzFBbWaN4dXjx5wCAiAXcOG3HeHlHY25SxyS3IR6hsqdhTSIy919fl/XnAVALy9vREXF6eSUGILCwtDWFgYN/OR3pOVlSHz0iWknT6N9HPn8OjmTQCAtY8PnIKC4D9hAmq1aAELV1eRk5Iu2HtrL5wtndHCpYXYUUiPKH2JjRYtWqBRo0b4+eefYfrXX3ylpaUYNWoUbty4gUuXLqklqBh4iQ3SR4+zspB68iRST5xA2rlzqCgqglmtWnBu2xbObduiduvWMK9dW+yYpIMahTVCe4/2+OnNn8SOQlpOrZfYWLVqFUJDQ+Hh4YGmTZsCAK5cuQKJRIK9e/e+XGIiElXhgwe4f/gwUg4fRmZMDADAsWlT+I0eDbeOHWHbsCFPOEhqdSfnDm5m3cT81+eLHYX0jNJFKCgoCElJSdiwYQNu3rwJQRAwYMAADB48GBY88oNIaxSlpuLegQO4d/AgHsXFQWpsDJd27dBm7ly4duoEU3t7sSOSHtl7ay+MDYzxhs8bYkchPfNSJ1Q0NzfHuHHjVJ2FiNTscVYWkiMjcW//fmRdvgwDU1O4deoEv1Gj4NqxIw9jJ9HsvbUXnet0hqUxjyykmvVSRWj9+vX48ccfkZiYiHPnzsHLywuLFy+Gj48P3nrrLVVnJKJXUFFSggfHjiFp926knTkDSCRwad8ebb//Hu6dO7P8kOgKSgtw8t5JLApZJHYU0kNKF6EffvgBc+bMwdSpUzFv3jzFkVV2dnZYsmQJixCRBhAEAZmXLiFp1y4kR0aivLAQjs2aoeVnn8EzJAQmtrZiRyRSOJp0FOXycp5NmkShdBFavnw5Vq9ejT59+uC7775TTG/ZsiWmT5+u0nBEpJzi9HQk7d6NxIgIFNy7Bws3N/gOHYo6oaGw/tv5wIg0ycE7B1Hfvj587HzEjkJ6SOkilJSUhObNm1ebbmJigqKiIpWEIqIXJysrw4Njx5CwYwfSz56F1NgYHt26IejLL+HUsiUkUqnYEYmeSRAEHEw4iN71e4sdhfSU0kXI29sbly9frnK2aQA4cOAA/Pz8VBaMiJ4v99YtJGzfjrt796I0NxeOzZqh1Zw58OrRg5eyIK1xK/sW7ubeRfd63cWOQnpK6SL08ccfY9KkSSgpKYEgCDh//jx+//13zJ8/Hz/9xJNgEalTWV4e7u7fj8SdO5Fz/TpMHRzg07cvfPr2hU3dumLHI1LawTsHYWxgjOA6wWJHIT2ldBEaOXIkKioqMGPGDBQXF2Pw4MFwc3PD0qVLMXDgQHVkJNJrcpkM6efOITEiAvePHIEgk8G1Y0e8Nn483Dp2hNTISOyIRC/tYMJBdPTqCAtjHr1I4lCqCFVUVGDjxo0IDQ3F2LFjkZWVBblcDicnJ3XlI9JbubduIWn3btzduxePMzNh7eODplOmoE7v3jCrVUvseESv7HH5Yxy/exzzOs8TOwrpMaWKkKGhId577z3cuHEDAODo6KiWUET6qig1FcmRkbi7dy8e3bwJE1tbePXsiTqhoXAICOBlLkinnLx3EiUVJdw/iESl9Kax1q1bIyYmptrO0kT0ch5nZSHl0CHc278fmTExkBobw61jRwRMmgSXDh1gYGwsdkQitTh45yDcrd3hV4sH2pB4lC5CEydOxEcffYT79+8jMDCw2vXFmjRporJwRLqq8P59pBw+jPtHjiAzJgYSAwO4tGuHtvPnw71LFx71RXohMiES3et255pOEpVEEARBmTdIn3JOEolEAkEQIJFIFGea1gX5+fmwsbFBXl4erK2txY5DWkxeUYHsq1eRevo0Hhw/jtz4eEiNjeHcti083ngD7l268GzPpFfu5d5DnaV1sK3/Nrzt97bYcUjHKPP9/VInVCSif1f44AEenj+PtNOnkX72LMry82FsYwOXDh3gP348XDp04HW+SG9FJkTCQGKA131eFzsK6TmlixD3DSKqThAEFNy7h8yYGGRERyMjOhpFqamARAIHf380GDIErq+9BvvGjSE1MBA7LpHoDt45iDbubWBrait2FNJzL3X1eQCIi4tDcnIyysrKqkx/8803XzkUkSYTBAElWVl4dPMmsmNjkXXlCrKvXkVZfj4gkcCuYUO4v/EGardqBafAQBjb2IgdmUijlMvKcTjxMGa0nyF2FCLli1BiYiL69u2L2NhYxb5BABQ7u+nSPkJEZXl5yL97F/mJichLSMCj+Hg8unkTpTk5AABjGxs4NGkC36FD4dCkCRwDAlh8iP7FufvnUFBWwMPmSSMoXYQ++OADeHt74/Dhw/Dx8cH58+eRnZ2Njz76CAsWLFBHRiK1kZWVoSQrC0VpaSi8fx9FDx6gMCUFhQ8eoODuXZRkZytea+HmBjtfX9QfOBB2vr6w8/WFhbs7j3ghUtLBOwfhaO6IFi4txI5CpHwROnfuHI4ePYpatWpBKpVCKpWiQ4cOmD9/PqZMmYKYmBh15CR6IYIgoKK4GGV5eSjLy0NpXh5KHz1CSXZ2ldvjzEwUp6dXKToAYOrgAAt3d1i6ucG5TRtYe3vD2tsbVl5eMDQzE+lTEemWqMQodPXpCqmk+lHIRDVN6SIkk8lg+dc5ThwdHZGamgpfX194eXkhPj5e5QFJdwiCAHl5OYSKCsjLyyErK6u8Ly+HvKwMstJSyEpLq/xcUVyMipISyEpKUPH4ceXj4mKUP7kvKkJ5QQHKCgpQXliI8vx8yCsqqo0tNTaGqYOD4mbXqBHcgoNh7uwMMycnWDg7w8LNjWWHSM2yi7NxMfUiJrWaJHYUIgAvUYT8/f1x9epV+Pj4oHXr1vjvf/8LY2NjhIeHw8fHRx0ZddKdrVvxKD4e+PtpnP76ucqJnZ48/+S5fzxWTPvr9uRn4clr/j7taa97ci+XKx4Lcjkgl1f+LJNBkMsV0+Ry+f9Pe8q9vKKi8ueKCsj/ugkyWWUBesn9x6SGhjAwM4OhqSkMzc0VNyMLC5jY2sLKwwNGlpYwsrKCkaUlTGxsYGxrCxNbW8XPRpaW3IRFpAGOJB2BAAFdfbqKHYUIwEsUoc8//xxFRUUAgHnz5qF379547bXX4ODggM2bN6s8oK7KS0xE5qVLAFD1C/rJz3+bVu0LXCKpnPbk9o/X/P05yd9fI5VWTvvrpJjVHkskkBoaVn2/gQEkUmn125Ppf91LDQ3//7GBAaSGhpXTDA0hMTCAgZERpEZGkBgaQmpkVFlujI0hNTaG1MgIBsbGlY9NTGBoagqpsTEMnvzMq6sT6YxDCYfgV8sPbtZuYkchAvASZ5Z+mpycHNjZ2encX9w8szQRkeoIgoA6S+ugb8O+WNJ9idhxSIep9czST2Nvb6+K2RARkQ67lX0LyXnJ3CxGGkXpItS5c+fnrvk5evToKwUiIiLdFJUYBSOpETrV6SR2FCIFpYtQs2bNqjwuLy/H5cuXce3aNQwfPlxVuYiISMccSjiEdh7tYGlsKXYUIgWli9DixYufOv3LL79EYWHhKwciIiLdUy4rx7G7x/BJh0/EjkJUhcrOZjVkyBD88ssvqpodERHpkD/u/4HCskLuH0QaR2VF6Ny5czA1NVXV7IiISIdEJUbB3syel9UgjaP0prF+/fpVeSwIAtLS0nDhwgXMnj1bZcGIiEh3HEo4hNe9X4eB1EDsKERVKF2EbP5xZW2pVApfX198/fXX6Natm8qCERGRbnj0+BGiU6MxpsUYsaMQVaN0EVqzZo06chARkY46mnQUckHO/YNII/HSv0REpFZRiVFo4NAAXrZeYkchqkbpNULKXEojJydH6UBERKRbDiUcQs/6PcWOQfRUSheh2bNnY968eQgJCUHbtm0BVB4xFhkZidmzZ/NyG0REpJCQk4Ck3CRuFiONpXQROnPmDL7++mtMnjxZMW3KlClYsWIFDh8+jIiICFXmIyIiLRaVGAUDiQGC6wSLHYXoqZTeRygyMhLdu3evNj0kJASHDx9WSSgiItINUYlRaO3eGjamNv/+YiIRKF2EHBwcsHPnzmrTIyIi4ODgoJJQRESk/WRyGY4mHeVmMdJoSm8a++qrrzB69GgcP35csY/QH3/8gYMHD+Knn35SeUAiItJOF1IvILckl0WINJrSRWjEiBFo1KgRli1bhh07dkAQBPj5+eHMmTNo3bq1OjISEZEWikqMgpWxFYLcgsSOQvRMShchAGjdujU2btyo6ixERKRDDiceRmfvzjAyMBI7CtEzKb2P0KVLlxAbG6t4vGvXLvTp0weffvopysrKVBqOiIi0U2FZIc6mnOVmMdJ4Sheh8ePH49atWwCAxMREDBgwAObm5ti6dStmzJih8oBERKR9Tt47iXJ5OYsQaTyli9CtW7fQrFkzAMDWrVvRqVMn/Pbbb1i7di22b9+u6nxERKSFohKi4GHtgQYODcSOQvRcShchQRAgl8sBAIcPH0bPnpWnTffw8EBWVpZq0xERkVaKSoxCV5+uL3xJJiKxKF2EWrZsiXnz5mH9+vU4ceIEevXqBQBISkpC7dq1VR6QiIi0S2pBKq5nXkfXutwsRppP6SK0ZMkSXLp0CZMnT8Znn32GevXqAQC2bduGdu3aqTwgERFpl8OJlVcZeN37dZGTEP07iSAIgipmVFJSAgMDAxgZ6c5hkvn5+bCxsUFeXh6sra3FjkNEpBWG7hyK6xnXcWn8JbGjkJ5S5vtb6TVCz2JqaqpTJYiIiJQnCAIOJx7m0WKkNVRWhIiIiK5lXEN6YTr3DyKtwSJEREQqE5UYBVNDU3Tw7CB2FKIXwiJEREQqE5UYhdc8X4OpoanYUYheCIsQERGpRElFCU7cPYGQuiFiRyF6YUpfdFUmk2Ht2rU4cuQIMjIyFCdXfOLo0aMqC0dERNrjdPJpPK54jG51u4kdheiFKV2EPvjgA6xduxa9evWCv7+/Vpw1tG/fvjh+/Dhef/11bNu2Tew4REQ6KfJOJFwsXeDv5C92FKIXpnQR2rRpE7Zs2aK4tIY2mDJlCkaNGoVff/1V7ChERDorMiES3ep204o/kImeUHofIWNjY8XZpLVF586dYWVlJXYMIiKdlVaQhtiMWO4fRFpH6SL00UcfYenSpVDRCalx8uRJhIaGwtXVFRKJBBEREdVes3LlSnh7e8PU1BSBgYE4deqUSsYmIiLVOJRwCADwhs8bIichUo7Sm8ZOnz6NY8eO4cCBA2jcuHG1s0nv2LFDqfkVFRWhadOmGDlyJN5+++1qz2/evBlTp07FypUr0b59e/z444/o0aMH4uLi4OnpCQAIDAxEaWlptfceOnQIrq6uSuUhIiLlHUo8hBYuLVDLopbYUYiUonQRsrW1Rd++fVUWoEePHujRo8czn1+0aBFGjx6NMWPGAKi86GtkZCR++OEHzJ8/HwBw8eJFlWQpLS2tUqjy8/NVMl8iIl0mF+Q4lHAIY1uMFTsKkdKULkJr1qxRR46nKisrw8WLFzFr1qwq07t164azZ8+qfLz58+fjq6++Uvl8iYh02eX0y8gqzuJh86SVNPqEillZWZDJZKhdu3aV6bVr10Z6evoLzyckJAT9+/fH/v374e7ujujo6Ke+7pNPPkFeXp7ilpKS8kr5iYj0QeSdSFgYWaCdRzuxoxApTek1QgCwbds2bNmyBcnJySgrK6vy3KVLl1QS7O/+eSimIAhKHZ4ZGRn5Qq8zMTGBiYmJUtmIiPTdocRD6OzdGcYGxmJHIVKa0muEli1bhpEjR8LJyQkxMTEICgqCg4MDEhMTn7uvz8twdHSEgYFBtbU/GRkZ1dYSERFRzSssK8SZ5DM8bJ60ltJFaOXKlQgPD8eKFStgbGyMGTNmICoqClOmTEFeXp5KwxkbGyMwMBBRUVFVpkdFRaFdO66CJSIS2/G7x1EuL+f+QaS1lN40lpycrCghZmZmKCgoAAAMHToUbdq0wYoVK5SaX2FhIe7cuaN4nJSUhMuXL8Pe3h6enp6YNm0ahg4dipYtW6Jt27YIDw9HcnIyJkyYoGx0IiJSscg7kahjWwf17euLHYXopShdhJydnZGdnQ0vLy94eXnhjz/+QNOmTZGUlPRSJ1m8cOECOnfurHg8bdo0AMDw4cOxdu1aDBgwANnZ2fj666+RlpYGf39/7N+/H15eXkqPRUREqnUo8RC6+fCyGqS9lC5CXbp0wZ49e9CiRQuMHj0aH374IbZt24YLFy6gX79+SgcIDg7+1wI1ceJETJw4Uel5ExGR+iQ9SsKt7FuY//p8saMQvTSli1B4eDjkcjkAYMKECbC3t8fp06cRGhrKzVVERHpk3+19MJIa8bIapNUkgqouGqaD8vPzYWNjg7y8PFhbW4sdh4hIo/Tc2BOlslIcGXZE7ChEVSjz/f1SJ1Q8deoUhgwZgrZt2+LBgwcAgPXr1+P06dMvMzuNExYWBj8/P7Rq1UrsKEREGqm4vBjH7h5Dz3o9xY5C9EqULkLbt29HSEgIzMzMEBMTo7g2V0FBAb799luVBxTDpEmTEBcX98wzUBMR6bujSUdRUlGCXg16iR2F6JUoXYTmzZuHVatWYfXq1VWuPN+uXTu1nFWaiIg0z/7b++Fj5wNfB1+xoxC9EqWLUHx8PDp27FhturW1NXJzc1WRiYiINJggCNh3ex961uvJw+ZJ6yldhFxcXKqcAPGJ06dPw8fHRyWhiIhIc13PvI7kvGRuFiOdoHQRGj9+PD744AP8+eefkEgkSE1NxcaNGzF9+nSe64eISA/sv70fZoZmCK4TLHYUolem9HmEZsyYgby8PHTu3BklJSXo2LEjTExMMH36dEyePFkdGYmISIPsu70Pr/u8DlNDU7GjEL2ylz6PUHFxMeLi4iCXy+Hn5wdLS0tVZxMdzyNERFRVbkkuHP/riBU9V2BCS55ElzSTMt/fSq8ResLc3BwtW7Z82bcTEZEWOpRwCDJBhp71ef4g0g0vXIRGjRr1Qq/75ZdfXjoMERFptn239yHAKQCeNp5iRyFSiRcuQmvXroWXlxeaN2/+UleZJyIi7SYX5Dhw+wBGNX+xP4yJtMELF6EJEyZg06ZNSExMxKhRozBkyBDY29urM5towsLCEBYWBplMJnYUIiKNcSH1AjKLM9GrPg+bJ93xwofPr1y5EmlpaZg5cyb27NkDDw8P/Oc//0FkZKTOrSHiJTaIiKrbd2sfbE1t0dajrdhRiFRGqfMImZiYYNCgQYiKikJcXBwaN26MiRMnwsvLC4WFherKSEREGiAiPgI96/eEofSlj7Mh0jgvdfV5AJBIJJBIJBAEAXK5XJWZiIhIw9zJuYOrD6/i7UZvix2FSKWUKkKlpaX4/fff0bVrV/j6+iI2NhYrVqxAcnKyTp5HiIiIKu28sRNmhmYIqRsidhQilXrh9ZsTJ07Epk2b4OnpiZEjR2LTpk1wcHBQZzYiItIQO27uQPd63WFhbCF2FCKVeuEzS0ulUnh6eqJ58+bPvdrwjh07VBZObDyzNBER8CD/AdwXu2N93/UY0mSI2HGI/pVaziw9bNiw5xYgIiLSTRE3I2AoNUTvBr3FjkKkckqdUJGIiPTPjps70MW7C2xNbcWOQqRyL33UGBER6b6s4iycuHsC/Rr2EzsKkVqwCBER0TPtid8DuSDHWw3fEjsKkVqwCBER0TPtuLkD7T3bw9nSWewoRGrBIvQUYWFh8PPzQ6tWrcSOQkQkmoLSAkQlRHGzGOk0FqGn4LXGiIiAA3cOoFRWir6N+oodhUhtWISIiOipdtzYgRYuLVDHto7YUYjUhkWIiIiqKakowb7b+7hZjHQeixAREVWzO343CssK0b9xf7GjEKkVixAREVWz/up6tHZrjQYODcSOQqRWLEJERFRFRlEGDtw+gGFNh4kdhUjtWISIiKiKTdc2QSqRYkDjAWJHIVI7FiEiIqpi3ZV16NWgFxzMHcSOQqR2LEJERKQQlxmHi2kXMbTJULGjENUIFiEiIlJYf2U97Ezt0Kt+L7GjENUIFiEiIgIAyAU5NsRuwIDGA2BiaCJ2HKIawSJEREQAgON3j+N+/n0eLUZ6hUWIiIgAVO4kXc++Htq4txE7ClGNYRF6Cl59noj0TVFZEbbf2I6hTYZCIpGIHYeoxrAIPQWvPk9E+ibiZgQKywoxpMkQsaMQ1SgWISIiQvilcLzm+Rp87HzEjkJUowzFDkBEROK6nH4ZJ++dxJZ3togdhajGcY0QEZGeW/bnMrhbu6Nvo75iRyGqcSxCRER6LLMoE7/F/oZJrSbBUMqNBKR/WISIiPTYjxd/hFQixdgWY8WOQiQKFiEiIj1VLivHyuiVGNJkCC+wSnqLRYiISE9ti9uGtMI0TGk9RewoRKJhESIi0lNL/1yKLt5d4O/kL3YUItFwzzgiIj305/0/8eeDP7Fr4C6xoxCJimuEiIj00LLzy+Bj54Ne9XuJHYVIVCxCRER6JjkvGVuub8HkVpNhIDUQOw6RqFiEiIj0zJfHv4S9mT3GBvKQeSLuI0REpEfiMuPw65VfsSRkCSyNLcWOQyQ6rhEiItIjnx/9HJ42nhgXOE7sKEQagWuEiIj0xJ/3/8TOmzuxrs86mBiaiB2HSCNwjdBThIWFwc/PD61atRI7ChGRSgiCgE+OfAJ/J38MDhgsdhwijSERBEEQO4Smys/Ph42NDfLy8mBtbS12HCKilxaVEIVuG7ph18BdeNP3TbHjEKmVMt/fXCNERKTj5IIcnxz5BG3d2yK0QajYcYg0CvcRIiLScdvituFi2kWcGHECEolE7DhEGoVrhIiIdNijx4/wYeSH6N2gNzp6dRQ7DpHGYREiItJhH0Z+iMKyQvzQ6wexoxBpJG4aIyLSUXtv7cWvV37FL2/+Andrd7HjEGkkrhEiItJBjx4/wrg949Czfk+MaDZC7DhEGotFiIhIB31w8AMUlxcjvHc4d5Ameg5uGiMi0jG743dj/dX1WPvWWrhZu4kdh0ijcY0QEZEOSS9Mx/i949Grfi8MazpM7DhEGo9FiIhIRxSXFyP091BIJVKEh3KTGNGL4KYxIiIdIJPL8O6Od3Ej8wZOjTwFVytXsSMRaQUWISIiHTAjagZ2x+/GroG70NyludhxiLQGixARkZZbGb0Si/5YhOU9lqN3g95ixyHSKtxHiIhIi+2J34P3D7yPqa2nYnLQZLHjEGkdFiEiIi217so69NvSD30a9sGCbgvEjkOklViEiIi0jCAImH9qPoZHDMeIpiOw+Z3NMJAaiB2LSCtxHyEiIi0ik8vwwcEPEBYdhi87fYk5nebwMHmiV8AiRESkJQpKCzBi1whE3IxAeO9wjA0cK3YkIq3HIkREpAVO3juJ4RHDkVWchYgBEQj1DRU7EpFO4D5CTxEWFgY/Pz+0atVK7ChEpOdKKkrw8aGPEbw2GO7W7rgy4QpLEJEKSQRBEMQOoany8/NhY2ODvLw8WFtbix2HiPRM9INojNo9Creyb2Fe53mY1nYad4omegHKfH9z0xgRkYa5k3MHnx/9HJuvb0bT2k0RPTYaTWo3ETsWkU5iESIi0hAZRRn4+sTX+PHij6htURs/hf6E4c2Gw1DKX9VE6sL/u4iIRBafFY/l55dj7eW1MJQaYl7neXi/9fswNzIXOxqRzmMRIiISgVyQ41DCISz9cykO3jkIJwsnfNT2I3zQ5gPYm9mLHY9Ib7AIERHVoBuZN7AxdiN+i/0NSblJaOHSAr/2+RUDGg+AiaGJ2PGI9A6LEBGRmt3JuYOImxH4LfY3xKTHwNbUFu80egcjmo1AO492PDM0kYhYhIiIVEwml+Hc/XPYE78He27twY2sGzAxMEHvBr0xu+Ns9Kzfk2t/iDQEixAR0SsSBAE3s27iSNIRHEk6guN3jyO3JBdOFk7oVb8XvunyDbrW7QpLY0uxoxLRP7AIEREpqUJegSvpV3Am5QzOpJzB6eTTSC1IhZHUCG092uLDNh+iW91uCHILglTCE/gTaTIWISKi5xAEAXdz7yI6NRrRD6JxIe0Coh9Eo6i8CMYGxmjl2gpDAoagi3cXdPDsAAtjC7EjE5ESWISIiP5SJivDjcwbuPLwCq6kX8Hlh5dxJf0Ksh9nAwA8bTzR0rUl5nSag/Ye7RHoGghTQ1ORUxPRq2ARIiK9U1hWiNvZt3Ez6ybiMuMQlxWHuMw43M6+DZkgAwD42Pmgae2meD/ofbR0bYmWri1R27K2yMmJSNVYhIhIJxWVFSHxUSLu5NxBwqME3Mm5g9s5txGfFY8HBQ8Ur3O2dEbjWo3R1acrpraeCn8nfwTUDoC1CS+0TKQPWISISCvJ5DI8KHiAxEeJSHqUVHmfW3mf+CgRD4seKl5raWyJunZ1Ud+hPkY0G4EGDg3g6+CLBg4NYGdmJ+KnICKxsQgRkcYqLCvEnZw7inKTkJOAxNzKn+/l3kO5vFzxWhdLF9S1ryw7IXVD4G3njXr29VDPvh5qmdfiSQuJ6KlYhIhIVBXyCiQ+SsTNrJuK25PNWOmF6YrXPVmr42Png7d834KPnQ+8bb3hbecNLxsvmBmZifgpiEhbsQgRUY14chh6bEYsYh/GVt5nxOJ29m3Fmh0rYyv4Ovqivn19BNcJRn37+qhnXw917etyrQ4RqQWLEBGpnCAISMpNQvSDaFxMu4iLaRdxKe0ScktyAQC2prYIcApAsFcwJraciEa1GqGhY0O4WLqw7BBRjWIRIqJXViYrQ/SDaJxNOYuz98/ibMpZZBRlAKg8904Llxb4qO1HaOHSAk1qN4GblRsLDxFpBBYhIlJahbwCF1Iv4FjSMRy9exRnks/gccVjWBhZIMgtCGNbjEVb97YIcgtCLYtaYsclInomFiEieiGpBak4eOcg9t/ej6jEKOSX5sPK2AodvTpibue5CK4TjKbOTWEo5a8VItIe/I1FRE8lCAIup1/Gzps7sTt+N648vAIJJGjj3gbT205H17pd0dK1JYsPEWk1/gYjIgW5IMfZlLPYHrcdEfERuJt7F7amtuhVvxdmtp+JbnW7wcHcQeyYREQqwyJEpOcEQcDFtIvYdG0TNl/fjPv59+Fi6YI+Dfugb8O+CK4TDCMDI7FjEhGpBYsQkZ5KyEnA+qvrseHqBiQ8SoCThRP6+/XHQP+BaOfRDlKJVOyIRERqxyL0FGFhYQgLC4NMJhM7CpFKPXr8CFuub8G6q+twNuUsrIyt8I7fO1jVexWC6wRzfx8i0jsSQRAEsUNoqvz8fNjY2CAvLw/W1rwSNWknmVyGI0lH8EvML4i4GYFyeTlC6oZgWNNheNP3TZgbmYsdkYhIpZT5/uaff0Q6KvFRItbErMGvV35FSn4KGjk2wrwu8/BuwLtwsXIROx4RkUZgESLSIY/LH2P7je34OeZnHL97HNYm1hjkPwgjm41EkFsQz+ZMRPQPLEJEWk4QBESnRmNNzBr8fu135JXmIbhOMNb3XY9+jfpx0xcR0XOwCBFpqfTCdKy/sh5rr6xFXGYc3K3dMTloMkY2G4m69nXFjkdEpBVYhIi0SFFZEXbF78KGqxtwKOEQDKWG6NuoLxaHLMbr3q/DQGogdkQiIq3CIkSk4cpl5TiadBQbYzdix40dKCovQnuP9ljRcwUGNB4AOzM7sSMSEWktFiEiDVQmK8ORxCPYGrcVETcj8KjkEXwdfDGrwywMDhgMHzsfsSMSEekEFiEiDZHzOAcH7xzEvtv7sP/2fuSW5KK+fX281/I99G/cH01rN+VRX0REKsYiRCSSCnkFLqRewLGkY9h/Zz/OppyFXJCjmXMzTG41Ge/4vYMmtZuw/BARqRGLEFENeVz+GDHpMTiTfAbH7h7DqeRTKCwrhKWxJbp4d8GqXqvQs35PuFm7iR2ViEhvsAgRqUFBaQHiMuMQmxGLC6kXcP7BeVx9eBUyQQYzQzN08OyATzt8is7enRHoEsiruxMRiYRFiOgllVSU4F7uPSQ8SkDio0QkPkrErexbuJZxDffy7gEAJJDAr5YfgtyCMC5wHILcghDgFMDiQ0SkIViEiADIBTkKywqRX5qPvJI85JXmIedxDrKKs5BdnI2s4ixkFGXgQcEDpBakIrUgFdmPsxXvNzYwhretN+rZ18OAxgPg7+QPfyd/NHRsCDMjMxE/GRERPQ+LkEiOJR1Dcl6yWuYtQFDNfIR/n8/TxnryvifP/fPxk2kChH+9lwtyxU0QBMgEGeSCHDL5X/eCDDK5DBXyClTIKyATZCiXlaNMVoZyeTnK5ZU/l1SUoLSiFKWyUpRWlKK4vBhF5UWV92WV98/6d7M2sYajuSMczR3hZuWGjl4d4WblBhcrF3jbesPHzgeuVq48mSERkRZiERLJ8vPLsfPmTrFj1DgJJIqjoJ72s1QiVfwsQeVjqUSqeGwgNYCBxAAGUgNIJVIYSAxgKDWEgfSve4kBjAyMYCQ1UtwbGxjD1NAUluaWMDU0hYmBCcyNzGFhZFF5b1x5b2NiAxtTG9iY2MDaxBoO5g6wN7OHsYGxmP9kRESkRixCItnaf6vK1tyokgT/fqj2Pw/nftp7eMg3ERFpAxYhkXAzChERkfikYgcgIiIiEguLEBEREektFiEiIiLSWyxCREREpLdYhIiIiEhvsQgRERGR3mIRIiIiIr3FIkRERER6i0WIiIiI9BaLEBEREektFiEiIiLSWyxCREREpLdYhIiIiEhv8erzzyEIAgAgPz9f5CRERET0op58bz/5Hn8eFqHnKCgoAAB4eHiInISIiIiUVVBQABsbm+e+RiK8SF3SU3K5HKmpqbCysoJEIhE7jkbKz8+Hh4cHUlJSYG1tLXYcvcfloXm4TDQLl4dmUdfyEAQBBQUFcHV1hVT6/L2AuEboOaRSKdzd3cWOoRWsra35S0WDcHloHi4TzcLloVnUsTz+bU3QE9xZmoiIiPQWixARERHpLRYheiUmJib44osvYGJiInYUApeHJuIy0SxcHppFE5YHd5YmIiIivcU1QkRERKS3WISIiIhIb7EIERERkd5iESIiIiK9xSJEREREeotFiGrMggUL0LhxY/j7+2PDhg1ix9F78fHxaNasmeJmZmaGiIgIsWPpNUNDQ8XyGDNmjNhx9FpBQQFatWqFZs2aISAgAKtXrxY7kt7r27cv7Ozs8M4776h0vjx8nmpEbGwshg8fjrNnzwIAXn/9dezbtw+2trbiBiMAQGFhIerUqYN79+7BwsJC7Dh6y9HREVlZWWLHIAAymQylpaUwNzdHcXEx/P39ER0dDQcHB7Gj6a1jx46hsLAQv/76K7Zt26ay+XKNENWIGzduoF27djA1NYWpqSmaNWuGgwcPih2L/rJ79268/vrrLEFEfzEwMIC5uTkAoKSkBDKZDFxvIK7OnTvDyspK5fNlESIAwMmTJxEaGgpXV1dIJJKnbiJZuXIlvL29YWpqisDAQJw6deqF5+/v749jx44hNzcXubm5OHr0KB48eKDCT6B71L1M/m7Lli0YMGDAKybWbTWxPPLz8xEYGIgOHTrgxIkTKkqum2pieeTm5qJp06Zwd3fHjBkz4OjoqKL0uqcmf1+pGq8+TwCAoqIiNG3aFCNHjsTbb79d7fnNmzdj6tSpWLlyJdq3b48ff/wRPXr0QFxcHDw9PQEAgYGBKC0trfbeQ4cOwc/PD1OmTEGXLl1gY2ODVq1awdCQ//k9j7qXiaurK4DKL98zZ85g06ZN6v1AWq4mlsfdu3fh6uqKa9euoVevXoiNjeUV0p+hJpaHra0trly5gocPH6Jfv3545513ULt2bbV/Nm1UU7+v1EIg+gcAws6dO6tMCwoKEiZMmFBlWsOGDYVZs2a91BijR48W9u7d+7IR9Y46l8m6deuEd99991Uj6pWa+H+ke/fuQnR09MtG1Cs1sTwmTJggbNmy5WUj6hV1Lo9jx44Jb7/99qtGrIKbxuhflZWV4eLFi+jWrVuV6d26dVPs/PwiMjIyAFQerXT+/HmEhISoNKc+UdUyAbhZTBVUsTwePXqk+Gv4/v37iIuLg4+Pj8qz6gNVLI+HDx8iPz8fQOVa05MnT8LX11flWfWBKn9fqQO3TdC/ysrKgkwmq7ZKuHbt2khPT3/h+fTp0we5ubmwsLDAmjVruGnsFahqmeTl5eH8+fPYvn27qiPqFVUsjxs3bmD8+PGQSqWQSCRYunQp7O3t1RFX56liedy/fx+jR4+GIAgQBAGTJ09GkyZN1BFX56nq91VISAguXbqEoqIiuLu7Y+fOnWjVqtUr5+M3Eb0wiURS5bEgCNWmPY8mNH9d86rLxMbGBg8fPlR1LL31KsujXbt2iI2NVUcsvfUqyyMwMBCXL19WQyr99aq/ryIjI1UdCQCPGqMX4OjoCAMDg2rNPSMjgzsOioTLRLNweWgWLg/NounLg0WI/pWxsTECAwMRFRVVZXpUVBTatWsnUir9xmWiWbg8NAuXh2bR9OXBTWMEoPLMwnfu3FE8TkpKwuXLl2Fvbw9PT09MmzYNQ4cORcuWLdG2bVuEh4cjOTkZEyZMEDG1buMy0SxcHpqFy0OzaPXyUOkxaKS1jh07JgCodhs+fLjiNWFhYYKXl5dgbGwstGjRQjhx4oR4gfUAl4lm4fLQLFwemkWblwevNUZERER6i/sIERERkd5iESIiIiK9xSJEREREeotFiIiIiPQWixARERHpLRYhIiIi0lssQkRERKS3WISIiIhIb7EIEVGNCg4OxtSpUzV2nDp16mDJkiUqz6OMu3fvQiKR8OrnRDWARYiInmnEiBHo06fPU5/LycnB+++/D19fX5ibm8PT0xNTpkxBXl5ezYYkInoFvOgqEb2U1NRUpKamYsGCBfDz88O9e/cwYcIEpKamYtu2bWLHIyJ6IVwjREQvxd/fH9u3b0doaCjq1q2LLl264JtvvsGePXtQUVHxwvPZsGEDWrZsCSsrKzg7O2Pw4MHIyMhQPH/8+HFIJBJERkaiefPmMDMzQ5cuXZCRkYEDBw6gUaNGsLa2xqBBg1BcXFxl3hUVFZg8eTJsbW3h4OCAzz//HH+/vGJGRgZCQ0NhZmYGb29vbNy4sVq+RYsWISAgABYWFvDw8MDEiRNRWFj4zM8zaNAgDBw4sMq08vJyODo6Ys2aNQCAgwcPokOHDopcvXv3RkJCwjPnuXbtWtja2laZFhERAYlEUmXanj17EBgYCFNTU/j4+OCrr75SalkQ6SMWISJSmby8PFhbW8PQ8MVXNpeVlWHu3Lm4cuUKIiIikJSUhBEjRlR73ZdffokVK1bg7NmzSElJwX/+8x8sWbIEv/32G/bt24eoqCgsX768ynt+/fVXGBoa4s8//8SyZcuwePFi/PTTT4rnR4wYgbt37+Lo0aPYtm0bVq5cWaWEAYBUKsWyZctw7do1/Prrrzh69ChmzJjxzM/z7rvvYvfu3VXKUmRkJIqKivD2228DAIqKijBt2jRER0fjyJEjkEql6Nu3L+Ry+Qv/u/1TZGQkhgwZgilTpiAuLg4//vgj1q5di2+++eal50mkF8S78D0Rabrhw4cLb7311gu9NisrS/D09BQ+++yz576uU6dOwgcffPDM58+fPy8AEAoKCgRBEIRjx44JAITDhw8rXjN//nwBgJCQkKCYNn78eCEkJKTKOI0aNRLkcrli2syZM4VGjRoJgiAI8fHxAgDhjz/+UDx/48YNAYCwePHiZ+bbsmWL4ODg8Mzny8rKBEdHR2HdunWKaYMGDRL69+//zPdkZGQIAITY2FhBEAQhKSlJACDExMQIgiAIa9asEWxsbKq8Z+fOncLff4W/9tprwrffflvlNevXrxdcXFyeOS4RCQLXCBHRK8vPz0evXr3g5+eHL774Qqn3xsTE4K233oKXlxesrKwQHBwMAEhOTq7yuiZNmih+rl27NszNzeHj41Nl2j/X5rRp06bK5qO2bdvi9u3bkMlkuHHjBgwNDdGyZUvF8w0bNqy2CerYsWPo2rUr3NzcYGVlhWHDhiE7OxtFRUVP/TxGRkbo37+/YjNbUVERdu3ahXfffVfxmoSEBAwePBg+Pj6wtraGt7f3Uz+zMi5evIivv/4alpaWitvYsWORlpZWbZMhEf0/7ixNRK+koKAA3bt3h6WlJXbu3AkjI6MXfm9RURG6deuGbt26YcOGDahVqxaSk5MREhKCsrKyKq/9+3wlEkm1cSQSiVKbloS/9hX65342f3fv3j307NkTEyZMwNy5c2Fvb4/Tp09j9OjRKC8vf+b73n33XXTq1AkZGRmIioqCqakpevTooXg+NDQUHh4eWL16NVxdXSGXy+Hv71/tMz8hlUqr7NsEoNr4crkcX331Ffr161ft/aamps/MSqTvWISI6KXl5+cjJCQEJiYm2L17t9JfuDdv3kRWVha+++47eHh4AAAuXLigsnx//PFHtcf169eHgYEBGjVqhIqKCly4cAFBQUEAgPj4eOTm5ipef+HCBVRUVGDhwoWQSitXoG/ZsuVfx23Xrh08PDywefNmHDhwAP3794exsTEAIDs7Gzdu3MCPP/6I1157DQBw+vTp586vVq1aKCgoQFFRESwsLACg2jmGWrRogfj4eNSrV+9f8xHR/2MRIqLnysvLq/ala29vDzs7O3Tr1g3FxcXYsGED8vPzkZ+fD6Dyi9vAwOBf5+3p6QljY2MsX74cEyZMwLVr1zB37lyVZU9JScG0adMwfvx4XLp0CcuXL8fChQsBAL6+vujevTvGjh2L8PBwGBoaYurUqTAzM1O8v27duqioqMDy5csRGhqKM2fOYNWqVf86rkQiweDBg7Fq1SrcunULx44dUzxnZ2cHBwcHhIeHw8XFBcnJyZg1a9Zz59e6dWuYm5vj008/xfvvv4/z589j7dq1VV4zZ84c9O7dGx4eHujfvz+kUimuXr2K2NhYzJs3T4l/NSI9I/ZOSkSkuYYPHy4AqHYbPny4Yifmp92SkpKeOc9/7iz922+/CXXq1BFMTEyEtm3bCrt3766yo/CTcR49eqR4z9N2Hv7iiy+Epk2bVhln4sSJwoQJEwRra2vBzs5OmDVrVpWdp9PS0oRevXoJJiYmgqenp7Bu3TrBy8urys7SixYtElxcXAQzMzMhJCREWLduXbU8T3P9+nUBgODl5VVlTEEQhKioKKFRo0aCiYmJ0KRJE+H48eMCAGHnzp2CIFTfWVoQKneOrlevnmBqair07t1bCA8PF/75K/zgwYNCu3btBDMzM8Ha2loICgoSwsPDn5uTSN9JBOEfG56JiIiI9ASPGiMiIiK9xSJEREREeotFiIiIiPQWixARERHpLRYhIiIi0lssQkRERKS3WISIiIhIb7EIERERkd5iESIiIiK9xSJEREREeotFiIiIiPQWixARERHprf8Dad2/cDS/2ugAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lambda_value, l2_mse_train, linewidth = 1, color = \"green\", label = \"Train\")\n",
    "plt.plot(lambda_value, l2_mse_test, linewidth = 1, color = \"brown\", label = \"Test\")\n",
    "plt.xlabel('L2 lambda value')\n",
    "plt.ylabel(\"Mean squared error\")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2eb2ca",
   "metadata": {},
   "source": [
    "### Discussion on λ, Model Complexity, and Error Rates\n",
    "\n",
    "The relationship between the regularization parameter $\\lambda$, model complexity, and error rates can be understood by observing the plotted Mean Squared Errors (MSE) for the training and testing datasets as a function of $\\lambda$.\n",
    "\n",
    "#### 1. **Effect of $\\lambda$ on Model Complexity:**\n",
    "\n",
    "- **Small $\\lambda$ values**: When $\\lambda$ is close to 0, the regularization effect is minimal. The model is primarily focused on minimizing the training error, which leads to a more complex model with larger weights. This increased model complexity can lead to **overfitting**, where the model fits the training data very well but fails to generalize to unseen data (higher test error).\n",
    "\n",
    "- **Large $\\lambda$ values**: As $\\lambda$ increases, the regularization term starts to dominate the loss function, penalizing large weights more strongly. This results in a simpler model with smaller weights, which reduces the model's flexibility. While this can reduce the risk of overfitting, it can also lead to **underfitting**, where the model is too simple to capture the underlying patterns in the data, leading to higher training and test errors.\n",
    "\n",
    "#### 2. **Underfitting and Overfitting:**\n",
    "\n",
    "- **Overfitting (Low $\\lambda$)**: When $\\lambda$ is small, the model is more complex and closely follows the noise in the training data. This is evident from the lower training error but higher test error, indicating poor generalization.\n",
    "\n",
    "- **Underfitting (High $\\lambda$)**: When $\\lambda$ is large, the model becomes too simple, potentially even linearizing complex relationships in the data. Both the training and test errors increase, indicating that the model is not capturing the necessary information from the data, resulting in underfitting.\n",
    "\n",
    "#### 3. **Optimal $\\lambda$:**\n",
    "\n",
    "- The optimal $\\lambda$ value strikes a balance between underfitting and overfitting, leading to a model that generalizes well to unseen data. This is typically observed where the test error is minimized, and the gap between training and test errors is reasonably small.\n",
    "\n",
    "#### 4. **Observations from the Plot:**\n",
    "\n",
    "- **Low $\\lambda$ Region**: In this region, you may observe that the training error is low, but the test error is relatively high, which is indicative of overfitting.\n",
    "\n",
    "- **High $\\lambda$ Region**: As $\\lambda$ increases, both training and test errors start to rise, signifying underfitting.\n",
    "\n",
    "- **Middle $\\lambda$ Region**: Somewhere in the middle, there is a sweet spot where the test error reaches its minimum, indicating the optimal balance between model complexity and regularization, thus reducing both underfitting and overfitting.\n",
    "\n",
    "In summary, choosing an appropriate $\\lambda$ value is crucial for achieving a model that generalizes well. Too small a $\\lambda$ might cause overfitting, while too large a $\\lambda$ can lead to underfitting. The goal is to find the $\\lambda$ value that minimizes the test error, indicating good generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069ae0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
